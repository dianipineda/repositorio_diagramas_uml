@startuml
skinparam style strict
skinparam component {
  BackgroundColor #F9F9F9
  BorderColor #555
  FontColor #333
}
skinparam arrow {
  Color #2C6EBE
}

title **Arquitectura de Flujo Ingesta Datalake**
left to right direction

package "Fuentes de datos" {
 package "Bases de datos Historicos" {
   package "BD Biosalc Historico" {
     left to right direction
     circle #black "Inicio" as ini_hist
     database "datos disponibles" as data_hist

     ini_hist -> data_hist
   }
 }
 package "Sistemas Core" {
   package "BD Biosalc - Oracle 19c" {
     left to right direction
    
     rectangle "Limpiar archivelogs obsoletos" as limpieza_bio
     cloud "¿⚠️Existen logs obsoletos?" as validador_bio
     circle #black "Inicio" as ini_bio
     circle "Fin" as fin_bio
     rectangle "Guardar archivelogs" as archivelogs_bio
     rectangle "Activar la captura" as activador_bio

     ini_bio -> activador_bio 
     activador_bio -> archivelogs_bio: genera logs
     ini_bio -> validador_bio
     validador_bio -> limpieza_bio: Si
     limpieza_bio -> fin_bio
     validador_bio -> fin_bio: No
    
   }  
   package "BD Querix 7 - Oracle 11g" {
     left to right direction
    
     rectangle "Limpiar archivelogs obsoletos" as limpieza_q7
     cloud "¿⚠️Existen logs obsoletos?" as validador_q7
     rectangle "Guardar archivelogs" as archivelogs_q7
     rectangle "Activar la captura" as activador_q7
     circle #black "Inicio" as ini_q7
     circle "Fin" as fin_q7

     ini_q7 -> activador_q7
     activador_q7 -> archivelogs_q7: genera logs
     ini_q7 -> validador_q7
     validador_q7 -> limpieza_q7: Si
     limpieza_q7 -> fin_q7
     validador_q7 -> fin_q7: No
   }  
   package "BD Siesa - SQLServer 2019" {
     left to right direction
    
     rectangle "Limpiar backups obsoletos" as limpieza_sie
     cloud "¿⚠️Existen backups obsoletos?" as validador_sie
     rectangle "Guardar backups" as backups_sie
     rectangle "Activar la captura" as activador_sie
     circle #black "Inicio" as ini_sie
     circle "Fin" as fin_sie

     ini_sie -> activador_sie
     activador_sie -> backups_sie: genera backups
     ini_sie -> validador_sie
     validador_sie -> limpieza_sie: Si
     limpieza_sie -> fin_sie
     validador_sie -> fin_sie: No
   }
 }
}


package "Servidor virtualizado (192.168.10.105)" {
  package "Contenedor Dockerizado (172.18.0.3)" {
    package "Plataforma Streaming (Kafka)" {
      package "Conector Fuente con \n JDBC para BD Biosalc" {
        left to right direction
        
        rectangle "Enviar eventos historicos \n (snapshot inicial) \n a Kafka" as carga_masiva_hist
        rectangle "Publicar eventos a \nlos tópicos de Kafka" as kafka_topics_hist
        
        carga_masiva_hist -> kafka_topics_hist
      }    
      package "Conector Fuente con \n Debezium para BD Biosalc" {
        left to right direction
        
        rectangle "Leer archivelogs \n con LogMiner de Oracle" as logminer_bio
        cloud "¿⚠️ Es la primera conexión?" as validador_snap_bio
        rectangle "Enviar eventos de cambios a Kafka" as eventos_cambios
        rectangle "Enviar eventos historicos \n (snapshot inicial) \n a Kafka" as eventos_historico
        rectangle "Publicar eventos a \nlos tópicos de Kafka" as kafka_topics_bio        
        
        logminer_bio -> validador_snap_bio
        validador_snap_bio --> eventos_historico: Si
        validador_snap_bio -> eventos_cambios: No
        eventos_historico -> kafka_topics_bio
        eventos_cambios -> kafka_topics_bio
      }      
      package "Conector Fuente con \n Debezium para BD Querix 7" {        
        rectangle "Leer archivelogs \n con LogMiner de Oracle" as logminer_q7
        cloud "¿⚠️ Es la primera conexión?" as validador_snap_q7
        rectangle "Enviar eventos de cambios a Kafka" as eventos_cambios_q7
        rectangle "Enviar eventos historicos \n (snapshot inicial) \n a Kafka" as eventos_historico_q7
        rectangle "Publicar eventos a \nlos tópicos de Kafka" as kafka_topics_q7
    
        logminer_q7 -> validador_snap_q7
        validador_snap_q7 --> eventos_historico_q7: Si
        validador_snap_q7 -> eventos_cambios_q7: No
        eventos_historico_q7 -> kafka_topics_q7
        eventos_cambios_q7 -> kafka_topics_q7
      }      
      package "Conector Fuente con \n Debezium para BD SIESA" {        
        rectangle "Leer cambios de BD \n SQLServer con CDC nativo" as cdc_siesa
        rectangle "Enviar eventos de cambios a Kafka" as eventos_cambios_sie
        rectangle "Publicar eventos a \nlos tópicos de Kafka" as kafka_topics_sie

        cdc_siesa -> eventos_cambios_sie
        eventos_cambios_sie -> kafka_topics_sie
      }      
      package "Conector Destino con \n Stream Reactor para S3 Minio" {
        left to right direction
        cloud "¿⚠️llegan eventos\ndesde kafka?" as validador_eventos_kafka_true
        circle "espera" as espera_eventos 
        cloud "¿⚠️Intento número\nn-1?" as validador_num_intento      
        rectangle "Consumir eventos" as eventos_a_minio
        rectangle "Manejar Error" as manejar_error
        rectangle "Recibir confirmación" as recep_commit
        rectangle "Preparar confirmación" as prep_commit_conn
 
        validador_eventos_kafka_true -> validador_num_intento: Si
        validador_eventos_kafka_true -> espera_eventos: No
        espera_eventos --> validador_eventos_kafka_true
        validador_num_intento -> eventos_a_minio: Si
        validador_num_intento -> manejar_error: No
        recep_commit -> prep_commit_conn
      }      
    }   
   package "Almacenamiento de información en MinIO" {
      left to right direction
      rectangle "Guardar temporalmente \neventos en RAM/disco duro " as write_temp
      rectangle "Escribir datos" as write_data
      database "data" as data
      note bottom of data #lightcyan
        **Particionamiento:**
        sistema origen/
        |- categoria de transformacion/
        |  |- tabla/
        |  |  |- año/
        |  |     |- mes/
      end note
      database "metadata" as metadata
      note bottom of metadata #lightcyan
        **Particionamiento:**
        sistema origen/
        |- categoria de transformacion/
        |  |- tabla/

      end note
      cloud "¿⚠️Escritura exitosa?" as validador_write_data

      write_temp -> write_data
      write_data -> validador_write_data
      write_data .> data

      ' Vínculo invisible para forzar la posición horizontal
      data .[#d0d0e1]. metadata
   }
   package "Programación y orquestación de\nflujos de trabajo con Airflow"{
      left to right direction
      circle "inicio" as ini_airflow
      rectangle "Ejecutar compactación" as compactacion_airflow
      rectangle "Ejecutar Limpieza de\narchivos obsoletos" as obsoletos_airflow
      note bottom of obsoletos_airflow #lightcyan
        **Archivos Obsoletos:**
        Archivos que iceberg
        usaba antes, pero que
        ya no necesita
      end note
      rectangle "Ejecutar Limpieza de\narchivos huérfanos" as huerfanos_airflow
      note bottom of huerfanos_airflow #lightcyan
        **Archivos Huérfanos:**
        Archivos que quedaron
        en el almacenamiento por
        un error y que nunca
        fueron parte de Iceberg
      end note
      rectangle "Ejecutar ingesta en postgres" as postgres_airflow
 
      ini_airflow -> compactacion_airflow
      ini_airflow -> obsoletos_airflow
      ini_airflow -> huerfanos_airflow
      ini_airflow -> postgres_airflow
   }
   package "Procesamiento de datos a gran escala con Apache Spark"{
      left to right direction
      rectangle "Solicitar compactación" as sol_compactacion_spark
      rectangle "Consolidar archivos pequeños" as consolidacion
      rectangle "Preparar confirmación" as prep_commit_from_spark
      consolidacion -> prep_commit_from_spark

      rectangle "Solicitar limpieza obsoletos" as sol_lim_obsoletos_spark
      rectangle "Limpiar archivos obsoletos" as limpiar_obsoletos
      limpiar_obsoletos -> prep_commit_from_spark

      rectangle "Solicitar limpieza huérfanos" as sol_lim_huerfanos_spark
      rectangle "Limpiar archivos huérfanos" as limpiar_huerfanos
      limpiar_huerfanos -> prep_commit_from_spark

      rectangle "Leer metadata" as leer_metadata_postgres
      rectangle "Leer data" as leer_data_postgres
      leer_metadata_postgres -> leer_data_postgres
      rectangle "preparar respuesta" as respuesta_postgres
      leer_data_postgres -> respuesta_postgres
      rectangle "conectar spark a\npostgres con\nconector JDBC" as conexion_postgres
      respuesta_postgres -> conexion_postgres
      rectangle "escribe respuesta" as write_postgres
      conexion_postgres -> write_postgres

    ' **Forzar la alineación horizontal**
      
   }
   package "Orquestacion de la lógica de procesamiento con Apache Iceberg"{
      left to right direction
      rectangle "Leer Metadata" as leer_metadata_iceberg
      rectangle "Filtrar archivos pequeños" as fil_pequenos
      rectangle "Filtrar archivos obsoletos" as fil_obsoletos
      rectangle "Filtrar archivos huérfanos" as fil_huerfanos
      rectangle "Recibir confirmación" as recep_commit_from_spark
      rectangle "Recibir confirmación" as recep_commit_from_connect
      rectangle "Escribir metadata" as write_metadata
      
      leer_metadata_iceberg -> fil_pequenos
      leer_metadata_iceberg -> fil_obsoletos
      leer_metadata_iceberg -> fil_huerfanos
      recep_commit_from_spark -> write_metadata
      recep_commit_from_connect -> write_metadata
   }
   package "Conversion de datos con ClickHouse" {
     left to right direction
     rectangle "Leer Metadata" as leer_metadata_ch
     rectangle "Leer Data" as leer_data_ch
     rectangle "Crear tabla telacional" as crear_tabla_ch
     database "tabla relacional" as tabla_ch
     leer_metadata_ch -> leer_data_ch
     leer_data_ch -> crear_tabla_ch  
     crear_tabla_ch .> tabla_ch
   }
  }  
}
package "Cliente (192.168.10.x)" {
   package "Consulta de datos" {
     left to right direction
      circle "inicio" as ini_cliente
      rectangle "Conectar con JDBC\na BD PostgreSQL" as conexion_postgres_client
      rectangle "Consultar datos" as consulta_postgres
      ini_cliente -> conexion_postgres_client: "Conexion a postgres"
      conexion_postgres_client -> consulta_postgres
      rectangle "Conectar con ODBC\n/JDBC a ClickHouse" as conexion_ch_client
      ini_cliente -> conexion_ch_client: "Conexion a ClickHouse"
      rectangle "Consultar tabla" as consulta_ch
      conexion_ch_client -> consulta_ch

   }
}
package "DB PostgreSQL (192.168.10.y)"{
   database "datos disponibles" as postgres
}

' Conexiones entre Sistemas Core y Plataforma Streaming
archivelogs_bio --> logminer_bio: "lee logs"
archivelogs_q7 --> logminer_q7: "lee logs"
backups_sie --> cdc_siesa: "lee cambios"
data_hist --> carga_masiva_hist: "lee cambios"

kafka_topics_bio --> validador_eventos_kafka_true: "envia eventos"
kafka_topics_q7 --> validador_eventos_kafka_true: "envia eventos"
kafka_topics_sie --> validador_eventos_kafka_true: "envia eventos"
kafka_topics_hist --> validador_eventos_kafka_true: "envia eventos"

manejar_error --> kafka_topics_bio: "envia error a tópico "Dead letter Queue"
manejar_error --> kafka_topics_q7: "envia error a tópico "Dead letter Queue"
manejar_error --> kafka_topics_sie: "envia error a tópico "Dead letter Queue"

eventos_a_minio --> write_temp: "envia eventos"
validador_write_data --> validador_num_intento: "No"
validador_write_data --> recep_commit: "Si"

postgres_airflow --> leer_metadata_postgres
compactacion_airflow --> sol_compactacion_spark
obsoletos_airflow --> sol_lim_obsoletos_spark
huerfanos_airflow --> sol_lim_huerfanos_spark

sol_compactacion_spark --> leer_metadata_iceberg: "Solicitud API Iceberg"
sol_lim_obsoletos_spark --> leer_metadata_iceberg: "Solicitud API Iceberg"
sol_lim_huerfanos_spark --> leer_metadata_iceberg: "Solicitud API Iceberg"

fil_pequenos --> consolidacion: "Respuesta API Iceberg"
fil_obsoletos --> limpiar_obsoletos: "Respuesta API Iceberg"
fil_huerfanos --> limpiar_huerfanos: "Respuesta API Iceberg"
prep_commit_from_spark --> recep_commit_from_spark
prep_commit_conn --> recep_commit_from_connect

write_postgres .> postgres

metadata .[#bb99ff].> leer_metadata_postgres
data .[#bb99ff].> leer_data_postgres
metadata .[#bb99ff].> leer_metadata_iceberg
write_metadata .[#bb99ff].> metadata

metadata .[#bb99ff].> leer_metadata_ch
data .[#bb99ff].> leer_data_ch
consulta_ch .[#bb99ff].> tabla_ch
consulta_postgres .[#bb99ff].> postgres
@enduml